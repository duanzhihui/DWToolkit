# 代码分类程序配置文件
# Code Classification Configuration
# 
# 积分制分类说明:
# - 每个正则规则可配置积分(score)，匹配时累加
# - 最终选择积分最高的类型作为代码类型
# - 只保留互斥规则（各类型独有的特征）

# 扫描的文件扩展名
scan_extensions:
  - ".py"
  - ".sql"
  - ".scala"
  - ".java"
  - ".sh"
  - ".hql"
  - ".hive"

# 文件过滤规则 (Unix通配符格式)
file_filter:
  include:
    - "*.py"
    - "*.sql"
    - "*.scala"
    - "*.java"
    - "*.sh"
    - "*.hql"
  exclude:
    - "*_test.py"
    - "test_*.py"
    - "__pycache__/*"
    - "*.pyc"

# 输出配置
output:
  path: "classification_result.xlsx"

# 分类规则 - 只保留互斥规则（各类型独有的特征）
# 规则格式: {pattern: "正则表达式", score: 积分}
classification_rules:
  hive:
    description: "Hive SQL及相关调用"
    priority: 10
    patterns:
      # Hive独有SQL关键字
      - {pattern: "\\bPARTITIONED\\s+BY\\b", score: 10}
      - {pattern: "\\bSTORED\\s+AS\\s+(ORC|PARQUET|TEXTFILE|AVRO|SEQUENCEFILE)\\b", score: 10}
      - {pattern: "\\bLOCATION\\s+['\"]hdfs://", score: 8}
      - {pattern: "\\bMSCK\\s+REPAIR\\s+TABLE\\b", score: 15}
      - {pattern: "\\bLOAD\\s+DATA\\s+(LOCAL\\s+)?INPATH\\b", score: 10}
      - {pattern: "\\bSET\\s+hive\\.\\w+", score: 15}
      # Hive独有函数
      - {pattern: "\\bget_json_object\\s*\\(", score: 10}
      - {pattern: "\\blateral\\s+view\\s+explode\\b", score: 12}
      - {pattern: "\\bcollect_set\\s*\\(", score: 8}
      - {pattern: "\\bcollect_list\\s*\\(", score: 8}
      - {pattern: "\\bposexplode\\s*\\(", score: 10}
      # Hive独有调用方式
      - {pattern: "HiveContext", score: 15}
      - {pattern: "hiveql", score: 12}
      - {pattern: "beeline\\s+-e", score: 15}
      - {pattern: "hive\\s+-e\\s+['\"]", score: 15}
      - {pattern: "execute_hive_query", score: 12}
      - {pattern: "run_hive_sql", score: 12}
      - {pattern: "HiveServer2", score: 15}
      - {pattern: "pyhive\\.hive", score: 15}
      - {pattern: "from\\s+pyhive\\s+import\\s+hive", score: 15}
      - {pattern: "impyla.*connect", score: 10}

  spark:
    description: "Spark及PySpark代码"
    priority: 20
    patterns:
      # Spark独有核心类
      - {pattern: "\\bSparkSession\\b", score: 20}
      - {pattern: "\\bSparkContext\\b", score: 20}
      - {pattern: "\\bSparkConf\\b", score: 15}
      - {pattern: "spark\\s*=\\s*SparkSession", score: 20}
      - {pattern: "sc\\s*=\\s*SparkContext", score: 20}
      # Spark独有DataFrame API
      - {pattern: "\\.createDataFrame\\s*\\(", score: 15}
      - {pattern: "\\.read\\s*\\.\\s*(csv|json|parquet|orc|jdbc|format)\\s*\\(", score: 12}
      - {pattern: "\\.write\\s*\\.\\s*(csv|json|parquet|orc|jdbc|format|mode)\\s*\\(", score: 12}
      - {pattern: "\\.withColumn\\s*\\(", score: 10}
      - {pattern: "\\.dropDuplicates\\s*\\(", score: 8}
      - {pattern: "\\.repartition\\s*\\(", score: 8}
      - {pattern: "\\.coalesce\\s*\\(", score: 8}
      - {pattern: "\\.cache\\s*\\(", score: 8}
      - {pattern: "\\.persist\\s*\\(", score: 8}
      # Spark独有RDD API
      - {pattern: "\\.parallelize\\s*\\(", score: 12}
      - {pattern: "\\.reduceByKey\\s*\\(", score: 12}
      - {pattern: "\\.groupByKey\\s*\\(", score: 10}
      - {pattern: "\\.sortByKey\\s*\\(", score: 10}
      - {pattern: "\\.saveAsTextFile\\s*\\(", score: 10}
      # Spark SQL独有
      - {pattern: "spark\\.sql\\s*\\(", score: 15}
      - {pattern: "\\.createOrReplaceTempView\\s*\\(", score: 12}
      - {pattern: "\\.createTempView\\s*\\(", score: 10}
      - {pattern: "\\.registerTempTable\\s*\\(", score: 10}
      # PySpark独有imports
      - {pattern: "from\\s+pyspark\\b", score: 20}
      - {pattern: "import\\s+pyspark\\b", score: 20}
      - {pattern: "pyspark\\.sql", score: 15}
      - {pattern: "pyspark\\.ml", score: 15}
      - {pattern: "pyspark\\.mllib", score: 15}
      - {pattern: "pyspark\\.streaming", score: 15}
      # Spark独有调用方式
      - {pattern: "spark-submit", score: 20}
      - {pattern: "spark2-submit", score: 20}
      - {pattern: "SparkSubmit", score: 15}
      - {pattern: "spark\\.jars", score: 10}
      - {pattern: "spark\\.executor", score: 12}
      - {pattern: "spark\\.driver", score: 12}
      - {pattern: "--master\\s+(yarn|local|spark://|mesos://|k8s://)", score: 15}
      - {pattern: "findspark\\.init", score: 15}

  flink:
    description: "Flink流处理及批处理代码"
    priority: 30
    patterns:
      # Flink独有环境类
      - {pattern: "\\bStreamExecutionEnvironment\\b", score: 25}
      - {pattern: "\\bExecutionEnvironment\\b", score: 20}
      - {pattern: "\\bTableEnvironment\\b", score: 20}
      - {pattern: "\\bStreamTableEnvironment\\b", score: 25}
      - {pattern: "\\bBatchTableEnvironment\\b", score: 20}
      - {pattern: "\\.getExecutionEnvironment\\s*\\(", score: 20}
      - {pattern: "\\.createLocalEnvironment\\s*\\(", score: 15}
      # Flink独有DataStream API
      - {pattern: "\\.addSource\\s*\\(", score: 15}
      - {pattern: "\\.addSink\\s*\\(", score: 15}
      - {pattern: "\\.keyBy\\s*\\(", score: 12}
      - {pattern: "\\.timeWindow\\s*\\(", score: 15}
      - {pattern: "\\.countWindow\\s*\\(", score: 15}
      - {pattern: "\\.assignTimestampsAndWatermarks\\s*\\(", score: 18}
      - {pattern: "\\.getSideOutput\\s*\\(", score: 12}
      # Flink独有Table API
      - {pattern: "\\.executeSql\\s*\\(", score: 15}
      - {pattern: "\\.sqlQuery\\s*\\(", score: 12}
      - {pattern: "\\.createTemporaryView\\s*\\(", score: 10}
      # Flink独有SQL语法
      - {pattern: "'connector'\\s*=", score: 15}
      - {pattern: "'format'\\s*=", score: 10}
      - {pattern: "WATERMARK\\s+FOR\\b", score: 20}
      - {pattern: "TUMBLE\\s*\\(", score: 18}
      - {pattern: "HOP\\s*\\(", score: 18}
      - {pattern: "SESSION\\s*\\(", score: 18}
      - {pattern: "MATCH_RECOGNIZE\\b", score: 20}
      # Flink独有imports
      - {pattern: "org\\.apache\\.flink\\b", score: 25}
      - {pattern: "from\\s+pyflink\\b", score: 25}
      - {pattern: "import\\s+pyflink\\b", score: 25}
      - {pattern: "pyflink\\.datastream", score: 20}
      - {pattern: "pyflink\\.table", score: 20}
      # Flink独有connectors
      - {pattern: "FlinkKafkaConsumer", score: 20}
      - {pattern: "FlinkKafkaProducer", score: 20}
      - {pattern: "JdbcSink", score: 15}
      - {pattern: "FileSink", score: 12}
      - {pattern: "FileSource", score: 12}
      # Flink独有调用方式
      - {pattern: "flink\\s+run\\b", score: 20}
      - {pattern: "flink-run", score: 15}
      - {pattern: "FlinkRunner", score: 15}
      - {pattern: "flink\\.execution", score: 12}
      - {pattern: "flink\\.parallelism", score: 12}
      - {pattern: "flink\\.checkpointing", score: 15}
      - {pattern: "state\\.backend", score: 12}
      - {pattern: "checkpoint\\.interval", score: 12}

  presto:
    description: "Presto查询"
    priority: 5
    patterns:
      - {pattern: "\\bpresto\\b.*\\bquery\\b", score: 15}
      - {pattern: "presto://", score: 20}
      - {pattern: "prestodb", score: 20}
      - {pattern: "from\\s+prestodb\\b", score: 20}
      - {pattern: "PrestoClient", score: 20}
      - {pattern: "presto-cli", score: 15}
      - {pattern: "--catalog\\s+\\w+\\s+--schema", score: 10}

  trino:
    description: "Trino查询"
    priority: 5
    patterns:
      - {pattern: "\\btrino\\b.*\\bquery\\b", score: 15}
      - {pattern: "trino://", score: 20}
      - {pattern: "from\\s+trino\\b", score: 20}
      - {pattern: "TrinoClient", score: 20}
      - {pattern: "trino-cli", score: 15}

  kafka:
    description: "Kafka消息队列"
    priority: 5
    patterns:
      - {pattern: "from\\s+kafka\\b", score: 20}
      - {pattern: "kafka-python", score: 15}
      - {pattern: "kafka://", score: 15}
      - {pattern: "confluent_kafka", score: 20}
